"""
å¤šçº¿ç¨‹OCRæœåŠ¡
æä¾›é«˜æ•ˆçš„å¹¶å‘OCRå¤„ç†èƒ½åŠ›
"""
import os
import time
import threading
import queue
import logging
import configparser
from typing import List, Dict, Optional, Union
from pathlib import Path
from django.conf import settings
from django.utils import timezone
import concurrent.futures
import traceback

from .ocr_service import OCRService
from ..models import OCRResult

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# è¯»å–é…ç½®
# config = configparser.ConfigParser()
# config.read(settings.BASE_DIR.parent / 'config.ini', encoding='utf-8')
config = settings.CFG._config

# OCRå¹¶å‘ç›¸å…³é…ç½®
OCR_MAX_WORKERS = config.getint('ocr', 'ocr_max_workers', fallback=4)


class MultiThreadOCR:
    """å¤šçº¿ç¨‹OCRå¤„ç†ç±»"""

    def __init__(self,
                 task,
                 lang: str = 'ch',
                 max_workers: int = None,
                 predict_save: bool = False, # æ˜¯å¦ä¿å­˜é¢„æµ‹å¯è§†åŒ–å›¾ç‰‡/JSON ç»“æœ
                 match_languages: Optional[List[str]] = None
                 ):
        """
        åˆå§‹åŒ–å¤šçº¿ç¨‹OCRå¤„ç†

        Args:
            lang (str): OCRè¯†åˆ«æ¨¡å‹è¯­è¨€ï¼ˆç”¨äº PaddleOCR åˆå§‹åŒ–ï¼Œä¾‹å¦‚ 'ch'ï¼‰ã€‚
            max_workers (int): æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ã€‚
            predict_save (bool): æ˜¯å¦ä¿å­˜é¢„æµ‹å¯è§†åŒ–å›¾ç‰‡/JSON ç»“æœã€‚
            match_languages (list[str] | None): åŠ¨æ€å‘½ä¸­åˆ¤å®šæ‰€ç”¨è¯­è¨€ç åˆ—è¡¨ï¼Œ
                è‹¥ä¸º None æˆ–ç©ºï¼Œåˆ™é»˜è®¤ ['ch']ã€‚å‘½ä¸­é€»è¾‘ä¸ OCR è¯†åˆ«è¯­è¨€è§£è€¦ã€‚
        """
        self.task = task
        # åˆå§‹åŒ– Redis åŠ©æ‰‹
        self.redis_helper = settings.REDIS
        self.lang = lang
        self.match_languages = match_languages or ['ch']
        self.max_workers = max_workers or OCR_MAX_WORKERS
        self.predict_save = predict_save  # æ˜¯å¦ä¿å­˜é¢„æµ‹å¯è§†åŒ–/JSON ç»“æœ
        # å®‰å…¨åˆå§‹åŒ–ä»»åŠ¡IDï¼šè‹¥æœªé€šè¿‡æ„é€ ä¼ å…¥ï¼Œåˆ™å°è¯•ä» task.id è·å–
        try:
            if not hasattr(self, 'task_id') or self.task_id is None:
                self.task_id = str(getattr(task, 'id', '')) if getattr(task, 'id', None) is not None else None
        except Exception:
            self.task_id = None

        # ç¡®ä¿æœ€å°å€¼ä¸º1ï¼Œé¿å…æ— æ•ˆå€¼
        if self.max_workers <= 0:
            self.max_workers = 1
            logger.warning("å·¥ä½œçº¿ç¨‹æ•°è®¾ç½®ä¸ºæ— æ•ˆå€¼ï¼Œå·²è‡ªåŠ¨è°ƒæ•´ä¸º1")

        # å•çº¿ç¨‹æ¨¡å¼ç‰¹æ®Šå¤„ç†
        if self.max_workers == 1:
            logger.warning("æ£€æµ‹åˆ°å•çº¿ç¨‹æ¨¡å¼é…ç½®(ocr_max_workers=1)ï¼Œå°†ç¦ç”¨GPUå¤šçº¿ç¨‹ä¼˜åŒ–")
            num_gpus = 0
            gpu_memory = 0
        else:
            # GPU ä¼˜å…ˆç­–ç•¥ï¼šè‹¥æ£€æµ‹åˆ°å¯ç”¨ GPUï¼Œåˆ™æŒ‰æ˜¾å­˜è§„æ¨¡åˆ†é…æ¯ä¸ª GPU çš„çº¿ç¨‹æ•°ï¼Œ
            # æ€»çº¿ç¨‹æ•° = GPU æ•°é‡ Ã— æ¯GPUçº¿ç¨‹æ•°ï¼›å¦åˆ™æŒ‰ CPU æ¨¡å¼ä½¿ç”¨ max_workersã€‚
            num_gpus = 0
            try:
                import torch
                if torch.cuda.is_available():
                    num_gpus = torch.cuda.device_count()
            except Exception:
                num_gpus = 0

            # æ£€æµ‹ GPU0 æ˜¾å­˜è§„æ¨¡ï¼ˆç”¨äºä¼°ç®—çº¿ç¨‹å¯†åº¦ï¼‰ã€‚å¦‚æ—  GPUï¼Œè¿”å› 0ã€‚
            gpu_memory = self._detect_gpu_memory()
            logger.warning(f"GPUæ˜¾å­˜å¤§å°(è®¾å¤‡0): {gpu_memory}MB")

        if num_gpus > 0 and self.max_workers > 1:
            threads_per_gpu = 1
            if gpu_memory >= 20000:
                threads_per_gpu = 16
                logger.warning(
                    f"æ£€æµ‹åˆ°é«˜ç«¯æ˜¾å¡(æ˜¾å­˜ {gpu_memory}MB)ï¼Œæ¯GPUåˆ†é…16ä¸ªçº¿ç¨‹"
                )
            elif gpu_memory >= 16000:
                threads_per_gpu = 8
                logger.warning(
                    f"æ£€æµ‹åˆ°é«˜æ€§èƒ½æ˜¾å¡(æ˜¾å­˜ {gpu_memory}MB)ï¼Œæ¯GPUåˆ†é…8ä¸ªçº¿ç¨‹"
                )
            elif gpu_memory >= 8000:
                threads_per_gpu = 4
                logger.warning(
                    f"æ£€æµ‹åˆ°ä¸­ç«¯æ˜¾å¡(æ˜¾å­˜ {gpu_memory}MB)ï¼Œæ¯GPUåˆ†é…4ä¸ªçº¿ç¨‹"
                )
            else:
                logger.warning(
                    f"æ£€æµ‹åˆ°å…¥é—¨çº§æ˜¾å¡(æ˜¾å­˜ {gpu_memory}MB)ï¼Œæ¯GPUåˆ†é…1ä¸ªçº¿ç¨‹"
                )

            total_gpu_threads = num_gpus * threads_per_gpu
            self.max_workers = min(self.max_workers, total_gpu_threads)
            logger.warning(
                f"GPUä¼˜å…ˆæ¨¡å¼: æ£€æµ‹åˆ° {num_gpus} ä¸ªGPUï¼Œæ¯ä¸ªGPU {threads_per_gpu} ä¸ªçº¿ç¨‹ï¼Œ"
                f"æ€»è®¡ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹"
            )
        else:
            if self.max_workers == 1:
                logger.warning(f"å•çº¿ç¨‹æ¨¡å¼: ä½¿ç”¨ 1 ä¸ªå·¥ä½œçº¿ç¨‹")
            else:
                logger.warning(f"CPUæ¨¡å¼: ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")

        # çŠ¶æ€è¿½è¸ª
        self.total_images = 0
        self.processed_images = 0
        self.error_count = 0
        self.is_running = False
        self.progress_callback = None
        self.result_callback = None

        # çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—å’Œé”
        self.image_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.lock = threading.Lock()

        logger.info(
            f"åˆå§‹åŒ–å¤šçº¿ç¨‹OCRæœåŠ¡ (è¯­è¨€: {self.lang}, æœ€å¤§çº¿ç¨‹: {self.max_workers})"
        )
        logger.info(f"é…ç½®æ–‡ä»¶ä¸­è®¾ç½®çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {OCR_MAX_WORKERS}")
        logger.info(f"å®é™…ä½¿ç”¨çš„å·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")

        # ä½¿ç”¨ç¼“å­˜æ± æœºåˆ¶ï¼Œé¿å…ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„OCRå®ä¾‹
        # å·¥ä½œçº¿ç¨‹å°†ä»å…±äº«çš„OCRå®ä¾‹æ± ä¸­è·å–å®ä¾‹ï¼Œå¤§å¤§æé«˜æ•ˆç‡
        self.worker_ocrs: List[Optional[OCRService]] = []
        
        logger.info("ä½¿ç”¨OCRå®ä¾‹æ± æ¨¡å¼åˆå§‹åŒ–å·¥ä½œçº¿ç¨‹")
        for i in range(self.max_workers):
            try:
                # åˆ›å»ºOCRServiceå®ä¾‹ï¼Œä½†ä¸ç«‹å³åˆå§‹åŒ–PaddleOCR
                # å®é™…çš„PaddleOCRå®ä¾‹å°†åœ¨å¤„ç†æ—¶ä»ç¼“å­˜æ± è·å–
                ocr_service = OCRService(lang=self.lang)
                
                # è‹¥ä»»åŠ¡é…ç½®æŒ‡å®šäº†é¢„è®¾ï¼Œåˆ™åº”ç”¨åˆ°æœåŠ¡é…ç½®ä¸­
                try:
                    preset_name = ''
                    if isinstance(self.task.config, dict):
                        preset_name = self.task.config.get('smart_ocr_preset', '')
                    if preset_name:
                        # åº”ç”¨é¢„è®¾åˆ°OCRServiceçš„å‚æ•°é…ç½®ä¸­
                        self._apply_preset_to_ocr_service(ocr_service, preset_name)
                except Exception as _preset_err:
                    logger.warning(f"åº”ç”¨æ™ºèƒ½OCRé¢„è®¾å¤±è´¥: {_preset_err}")
                
                self.worker_ocrs.append(ocr_service)
                logger.debug(f"åˆå§‹åŒ–å·¥ä½œçº¿ç¨‹ {i} çš„OCRæœåŠ¡æˆåŠŸ")
                
            except Exception as init_err:
                logger.error(f"åˆå§‹åŒ–OCRæœåŠ¡å¤±è´¥(çº¿ç¨‹ç´¢å¼• {i}): {init_err}")
                self.worker_ocrs.append(None)
        
        # é¢„çƒ­ç¼“å­˜ï¼šå¦‚æœé…ç½®å¯ç”¨ï¼Œé¢„å…ˆåˆ›å»ºå¸¸ç”¨çš„OCRå®ä¾‹
        if config.getboolean('ocr', 'ocr_warm_cache_on_startup', fallback=False):
            self._warm_ocr_cache()

    def _apply_preset_to_ocr_service(self, ocr_service: OCRService, preset_name: str) -> None:
        """
        åº”ç”¨OCRé¢„è®¾åˆ°OCRServiceå®ä¾‹
        
        Args:
            ocr_service: OCRServiceå®ä¾‹
            preset_name: é¢„è®¾åç§° (high_speed/balanced/high_precision)
        """
        preset = preset_name.lower()
        
        if preset == 'high_speed':
            # é«˜é€Ÿæ¨¡å¼ï¼šé™ä½ç²¾åº¦æ¢é€Ÿåº¦
            ocr_service.text_det_limit_type = 'max'
            ocr_service.text_det_limit_side_len = 960
            ocr_service.text_det_thresh = 0.5
            ocr_service.text_det_box_thresh = 0.7
            ocr_service.text_det_unclip_ratio = 1.0
            ocr_service.smart_ocr_dynamic_limit_enabled = False
            logger.info(f"åº”ç”¨é«˜é€Ÿæ¨¡å¼é¢„è®¾åˆ°OCRæœåŠ¡")
            
        elif preset == 'high_precision':
            # é«˜ç²¾åº¦æ¨¡å¼ï¼šæå‡ç²¾åº¦
            ocr_service.text_det_limit_type = 'min'
            ocr_service.text_det_limit_side_len = 1280
            ocr_service.text_det_thresh = 0.2
            ocr_service.text_det_box_thresh = 0.4
            ocr_service.text_det_unclip_ratio = 2.0
            ocr_service.smart_ocr_dynamic_limit_enabled = False
            logger.info(f"åº”ç”¨é«˜ç²¾åº¦æ¨¡å¼é¢„è®¾åˆ°OCRæœåŠ¡")
            
        else:
            # balanced æˆ–å…¶ä»–ï¼šå‡è¡¡æ¨¡å¼
            ocr_service.text_det_limit_side_len = 960
            ocr_service.text_det_thresh = 0.3
            ocr_service.text_det_box_thresh = 0.6
            ocr_service.text_det_unclip_ratio = 1.5
            ocr_service.smart_ocr_dynamic_limit_enabled = True
            logger.info(f"åº”ç”¨å‡è¡¡æ¨¡å¼é¢„è®¾åˆ°OCRæœåŠ¡")
    
    def _warm_ocr_cache(self) -> None:
        """
        æ™ºèƒ½é¢„çƒ­OCRç¼“å­˜æ± ï¼Œé¢„å…ˆåˆ›å»ºæœ€å¸¸ç”¨çš„OCRå®ä¾‹
        
        é¢„çƒ­ç­–ç•¥ï¼š
        1. å¦‚æœç¦ç”¨åŠ¨æ€åˆ‡æ¢ï¼Œåªé¢„çƒ­é»˜è®¤é…ç½®ï¼ˆ1ä¸ªå®ä¾‹=2æ¬¡æ¨¡å‹åˆ›å»ºï¼‰
        2. å¦‚æœå¯ç”¨åŠ¨æ€åˆ‡æ¢ï¼Œé¢„çƒ­maxå’Œminä¸¤ç§é…ç½®ï¼ˆ2ä¸ªå®ä¾‹=4æ¬¡æ¨¡å‹åˆ›å»ºï¼‰
        """
        try:
            logger.info("å¼€å§‹æ™ºèƒ½é¢„çƒ­OCRç¼“å­˜æ± ")
            
            # ä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„OCRæœåŠ¡è·å–å®ä¾‹æ± 
            valid_ocr_service = None
            for ocr_service in self.worker_ocrs:
                if ocr_service is not None:
                    valid_ocr_service = ocr_service
                    break
            
            if valid_ocr_service is None:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„OCRæœåŠ¡ï¼Œè·³è¿‡ç¼“å­˜é¢„çƒ­")
                return
            
            # åŸºç¡€é…ç½®å‚æ•°
            base_config = {
                'lang': self.lang,
                'text_det_thresh': valid_ocr_service.text_det_thresh,
                'text_det_box_thresh': valid_ocr_service.text_det_box_thresh,
                'text_det_unclip_ratio': valid_ocr_service.text_det_unclip_ratio,
                'text_det_limit_side_len': valid_ocr_service.text_det_limit_side_len,
            }
            
            # æ ¹æ®æ˜¯å¦å¯ç”¨åŠ¨æ€åˆ‡æ¢å†³å®šé¢„çƒ­ç­–ç•¥
            if valid_ocr_service.smart_ocr_dynamic_limit_enabled:
                # å¯ç”¨åŠ¨æ€åˆ‡æ¢ï¼šé¢„çƒ­maxå’Œminä¸¤ç§é…ç½®
                configs_to_warm = [
                    {**base_config, 'limit_type': 'max'},  # å®½å›¾ä¼˜åŒ–
                    {**base_config, 'limit_type': 'min'},  # é«˜å›¾ä¼˜åŒ–
                ]
                logger.info("åŠ¨æ€åˆ‡æ¢å·²å¯ç”¨ï¼Œé¢„çƒ­maxå’Œminä¸¤ç§é…ç½®")
            else:
                # ç¦ç”¨åŠ¨æ€åˆ‡æ¢ï¼šåªé¢„çƒ­é»˜è®¤é…ç½®
                configs_to_warm = [
                    {**base_config, 'limit_type': valid_ocr_service.text_det_limit_type}
                ]
                logger.info(f"åŠ¨æ€åˆ‡æ¢å·²ç¦ç”¨ï¼Œåªé¢„çƒ­é»˜è®¤é…ç½®: {valid_ocr_service.text_det_limit_type}")
            
            # æ‰§è¡Œé¢„çƒ­
            warmed_count = 0
            for config in configs_to_warm:
                try:
                    valid_ocr_service.ocr_pool.get_ocr_instance(**config)
                    warmed_count += 1
                    logger.info(f"âœ… é¢„çƒ­OCRå®ä¾‹æˆåŠŸ: {config['limit_type']} (ç¬¬{warmed_count}ä¸ª)")
                except Exception as e:
                    logger.warning(f"âŒ é¢„çƒ­OCRå®ä¾‹å¤±è´¥: {config['limit_type']}, é”™è¯¯: {e}")
            
            # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
            cache_info = valid_ocr_service.ocr_pool.get_cache_info()
            logger.info(f"ğŸ¯ OCRç¼“å­˜æ± é¢„çƒ­å®Œæˆ: é¢„çƒ­{warmed_count}ä¸ªå®ä¾‹, é¢„è®¡åˆ›å»º{warmed_count*2}ä¸ªæ¨¡å‹, ç¼“å­˜ç»Ÿè®¡: {cache_info}")
            
        except Exception as e:
            logger.error(f"OCRç¼“å­˜æ± é¢„çƒ­å¤±è´¥: {e}")

    def _detect_gpu_memory(self) -> int:
        """
        æ£€æµ‹GPUæ˜¾å­˜å¤§å°(MB)ã€‚

        è¿”å›:
            int: æ˜¾å­˜å¤§å°(MB)ã€‚æ— GPUæˆ–æ£€æµ‹å¤±è´¥æ—¶è¿”å›0ã€‚
        """
        try:
            import torch
            # ä»…åšå¿«é€Ÿå¯ç”¨æ€§æ£€æµ‹ï¼Œä¸åšå¤æ‚çš„ CUDA åŠ è½½
            if torch.cuda.is_available():
                gpu_properties = torch.cuda.get_device_properties(0)
                total_memory = gpu_properties.total_memory / (1024 * 1024)
                logger.warning(
                    f"ä½¿ç”¨NVIDIA GPU: {gpu_properties.name}ï¼Œæ˜¾å­˜: {total_memory:.0f}MB"
                )
                return int(total_memory)
            logger.warning("æœªæ£€æµ‹åˆ°NVIDIA GPU")
            return 0
        except Exception as e:
            logger.warning(f"æ£€æµ‹GPUæ˜¾å­˜å¤±è´¥: {str(e)}")
            return 0

    def set_callbacks(self, progress_callback=None, result_callback=None):
        """
        è®¾ç½®å›è°ƒå‡½æ•°

        Args:
            progress_callback: è¿›åº¦å›è°ƒï¼Œæ¥æ”¶å‚æ•°(processed, total, error_count)
            result_callback: ç»“æœå›è°ƒï¼Œæ¥æ”¶å‚æ•°(result)
        """
        self.progress_callback = progress_callback
        self.result_callback = result_callback

    def _collect_image_paths(self, image_dir: str, image_formats: List[str]) -> List[str]:
        """
        æ”¶é›†ç›®å½•ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œç»Ÿä¸€è¿”å›ç›¸å¯¹è·¯å¾„

        Args:
            image_dir: å›¾ç‰‡ç›®å½•
            image_formats: å›¾ç‰‡æ ¼å¼åˆ—è¡¨

        Returns:
            List[str]: ç›¸å¯¹äºMEDIA_ROOTçš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        image_extensions = set()
        for ext in image_formats:
            image_extensions.add(f".{ext.lower()}")
            image_extensions.add(f".{ext.upper()}")

        image_paths = []
        media_root = settings.MEDIA_ROOT

        # ç¡®ä¿media_rootä»¥/ç»“å°¾
        if not media_root.endswith(os.sep):
            media_root += os.sep

        logger.info(f"åª’ä½“æ ¹ç›®å½•: {media_root}")
        logger.info(f"æœç´¢ç›®å½•: {image_dir}")

        for path in Path(image_dir).rglob("*"):
            if path.is_file() and path.suffix in image_extensions:
                absolute_path = str(path.resolve())

                # ç»Ÿä¸€è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                if absolute_path.startswith(media_root):
                    relative_path = os.path.relpath(absolute_path, media_root)
                    relative_path = relative_path.replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
                    image_paths.append(relative_path)
                    logger.debug(f"æ·»åŠ å›¾ç‰‡: {relative_path}")
                else:
                    logger.warning(f"è·³è¿‡ä¸åœ¨åª’ä½“ç›®å½•ä¸‹çš„å›¾ç‰‡: {absolute_path}")

        logger.info(f"æ”¶é›†åˆ° {len(image_paths)} å¼ å›¾ç‰‡")
        return image_paths

    def _get_full_image_path(self, relative_path: str) -> str:
        """
        æ ¹æ®ç›¸å¯¹è·¯å¾„è·å–å®Œæ•´çš„ç»å¯¹è·¯å¾„

        Args:
            relative_path: ç›¸å¯¹äºMEDIA_ROOTçš„è·¯å¾„

        Returns:
            str: å®Œæ•´çš„ç»å¯¹è·¯å¾„
        """
        return os.path.join(settings.MEDIA_ROOT, relative_path)

    def recognize_batch(self, image_dir: str, image_formats: List[str] = None) -> List[Dict]:
        """
        å¤šçº¿ç¨‹æ‰¹é‡è¯†åˆ«å›¾ç‰‡

        Args:
            image_dir: å›¾ç‰‡ç›®å½•
            image_formats: å›¾ç‰‡æ ¼å¼åˆ—è¡¨

        Returns:
            List[Dict]: è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        all_results = []
        batch_start_time = time.time()

        if image_formats is None:
            image_formats = ['jpg', 'jpeg', 'png', 'bmp', 'tiff', 'tif', 'webp']

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡è·¯å¾„
        logger.warning(f"å¼€å§‹æ”¶é›†å›¾ç‰‡è·¯å¾„: {image_dir}")
        path_collect_start = time.time()
        image_paths = self._collect_image_paths(image_dir, image_formats)
        path_collect_time = time.time() - path_collect_start

        self.total_images = len(image_paths)
        self.processed_images = 0
        self.error_count = 0
        self.is_running = True

        # åˆå§‹åŒ–redis
        self.init_progress(self.total_images)

        if not image_paths:
            logger.warning(f"åœ¨ç›®å½• {image_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡")
            return []

        # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªOCRå®ä¾‹ä¸å¯ç”¨ï¼Œç»™å‡ºå‘Šè­¦ä½†ç»§ç»­æ‰§è¡Œï¼ˆå¯¹åº”çº¿ç¨‹å°†è·³è¿‡ï¼‰
        if all(inst is None for inst in self.worker_ocrs):
            logger.error("æ‰€æœ‰OCRå®ä¾‹åˆå§‹åŒ–å‡å¤±è´¥ï¼Œæ— æ³•å¼€å§‹è¯†åˆ«")
            return []

        # å¦‚æœå›¾ç‰‡æ•°é‡è¿‡å¤šï¼Œæç¤ºå¤„ç†æ—¶é—´å¯èƒ½è¾ƒé•¿
        if len(image_paths) > 10000:
            logger.warning(f"å›¾ç‰‡æ•°é‡è¿‡å¤š({len(image_paths)}å¼ )ï¼Œå¤„ç†æ—¶é—´å¯èƒ½è¾ƒé•¿")

        logger.warning(
            f"åœ¨ {image_dir} åŠå…¶å­ç›®å½•ä¸­æ‰¾åˆ° {self.total_images} å¼ å›¾ç‰‡ï¼Œ"
            f"æ”¶é›†è€—æ—¶ {path_collect_time:.2f} ç§’"
        )
        logger.warning(f"ä½¿ç”¨ {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œå¤„ç†")

        # ç²—ç•¥ä¼°è®¡å¤„ç†æ—¶é—´ï¼ˆä»…ç”¨äºæ—¥å¿—å‹å¥½æç¤ºï¼‰
        est_time_per_image = 0.5
        est_total_time = (
            (self.total_images / self.max_workers) * est_time_per_image
            if self.max_workers > 0 else 0
        )
        est_hours = int(est_total_time / 3600)
        est_minutes = int((est_total_time % 3600) / 60)
        est_seconds = int(est_total_time % 60)
        logger.warning(
            f"é¢„è®¡å¤„ç†æ—¶é—´: {est_hours}å°æ—¶ {est_minutes}åˆ†é’Ÿ {est_seconds}ç§’"
        )

        # å°†å›¾ç‰‡è·¯å¾„æ”¾å…¥é˜Ÿåˆ—
        for path in image_paths:
            self.image_queue.put(path)

        # å¯åŠ¨ç»“æœæ”¶é›†çº¿ç¨‹
        collector_thread = threading.Thread(target=self._result_collector, args=(all_results,))
        collector_thread.start()

        # å•çº¿ç¨‹æ¨¡å¼ç‰¹æ®Šå¤„ç†
        if self.max_workers == 1:
            logger.warning("ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼å¤„ç†å›¾ç‰‡ï¼Œé¿å…å¹¶å‘é—®é¢˜")
            self._process_single_thread()
        else:
            # å¤šçº¿ç¨‹æ¨¡å¼å¤„ç†
            self._process_multi_thread()

        # ç­‰å¾…æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œ
        self.is_running = False
        self.result_queue.join()
        collector_thread.join()

        # å®Œæˆä»»åŠ¡
        self.finish_progress('completed')

        # ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - batch_start_time
        avg_speed = self.processed_images / total_time if total_time > 0 else 0
        hours = int(total_time / 3600)
        minutes = int((total_time % 3600) / 60)
        seconds = total_time % 60  # ä¿ç•™å°æ•°

        logger.warning(
            f"{'å•' if self.max_workers == 1 else 'å¤š'}çº¿ç¨‹å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {self.processed_images} å¼ å›¾ç‰‡ï¼Œé”™è¯¯ "
            f"{self.error_count} å¼ ï¼ŒæˆåŠŸç‡: "
            f"{(self.processed_images - self.error_count) / self.total_images:.2%}"
        )
        logger.warning(
            f"æ€»å¤„ç†æ—¶é—´: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’ï¼Œå¹³å‡é€Ÿåº¦: "
            f"{avg_speed:.2f} å¼ /ç§’"
        )

        return all_results

    def _process_single_thread(self):
        """å•çº¿ç¨‹å¤„ç†é€»è¾‘ï¼ˆç»Ÿä¸€å¤ç”¨ _process_single_imageï¼‰
        
        è¯´æ˜:
            æ—©å‰å®ç°ä¸­éƒ¨åˆ†å›¾ç‰‡æœªç»è¿‡ `_process_single_image`ï¼Œ
            å¯¼è‡´ `languages/has_match` æœªè®¾ç½®ä¸”æœªæ›´æ–°è¿›åº¦ç»Ÿè®¡ã€‚
            æ­¤å¤„ç»Ÿä¸€æ”¹ä¸ºæ‰€æœ‰å›¾ç‰‡å‡èµ° `_process_single_image`ï¼Œ
            ç¡®ä¿å‘½ä¸­åˆ¤å®šä¸ Redis è¿›åº¦è®¡æ•°ä¸€è‡´ã€‚
        """
        try:
            ocr = self.worker_ocrs[0]
            if ocr is None:
                logger.error("OCRå®ä¾‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¤„ç†å›¾ç‰‡")
                return

            while not self.image_queue.empty():
                try:
                    relative_path = self.image_queue.get(block=False)
                    # ç»Ÿä¸€ä½¿ç”¨å•å¼ å›¾ç‰‡å¤„ç†å‡½æ•°ï¼Œå†…éƒ¨è´Ÿè´£è®¾ç½® has_match/languages
                    self._process_single_image(ocr, relative_path, 0)
                except queue.Empty:
                    break
                except Exception as e:
                    self.update_progress_exception()
                    logger.error(f"å•çº¿ç¨‹å¤„ç†å¼‚å¸¸: {e}")
                    logger.error(traceback.format_exc())
                    break
        except Exception as e:
            self.update_progress_exception()
            logger.error(f"å•çº¿ç¨‹å¤„ç†é€»è¾‘å¤±è´¥: {e}")
            logger.error(traceback.format_exc())

    def _process_multi_thread(self):
        """å¤šçº¿ç¨‹å¤„ç†é€»è¾‘"""
        progress_reporting_active = True

        def progress_reporter():
            last_processed = 0
            last_time = time.time()

            while progress_reporting_active and self.is_running:
                time.sleep(30)
                current_time = time.time()
                current_processed = self.processed_images
                if current_processed > last_processed:
                    time_diff = current_time - last_time
                    images_diff = current_processed - last_processed
                    speed = images_diff / time_diff if time_diff > 0 else 0
                    remaining_images = self.total_images - current_processed
                    est_remaining_time = (
                        remaining_images / speed if speed > 0 else 0
                    )
                    est_h = int(est_remaining_time / 3600)
                    est_m = int((est_remaining_time % 3600) / 60)
                    est_s = int(est_remaining_time % 60)
                    progress = (
                        (current_processed / self.total_images) * 100
                        if self.total_images > 0 else 0
                    )
                    logger.warning(
                        f"è¿›åº¦: {current_processed}/{self.total_images} "
                        f"({progress:.2f}%), é€Ÿåº¦: {speed:.2f} å¼ /ç§’, å‰©ä½™æ—¶é—´: "
                        f"{est_h}å°æ—¶ {est_m}åˆ†é’Ÿ {est_s}ç§’"
                    )
                    last_processed = current_processed
                    last_time = current_time

        # å¯åŠ¨è¿›åº¦æŠ¥å‘Šçº¿ç¨‹
        progress_thread = threading.Thread(target=progress_reporter)
        progress_thread.daemon = True
        progress_thread.start()

        try:
            # åˆ›å»ºçº¿ç¨‹æ± 
            logger.warning(f"åˆ›å»ºçº¿ç¨‹æ± ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                for i in range(self.max_workers):
                    logger.warning(f"åˆ›å»ºå·¥ä½œçº¿ç¨‹ {i}")
                    futures.append(executor.submit(self._worker_thread, i))

                # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
                logger.warning("ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ")
                self.update_status('running')
                concurrent.futures.wait(futures)
                logger.warning("æ‰€æœ‰å·¥ä½œçº¿ç¨‹å·²å®Œæˆ")

        except Exception as e:
            self.update_progress_exception()
            logger.error(f"å¤šçº¿ç¨‹å¤„ç†å¼‚å¸¸: {str(e)}")
            logger.error(traceback.format_exc())
        finally:
            progress_reporting_active = False

    def _process_single_image(self, ocr: OCRService, relative_path: str, worker_id: int):
        logger.debug(f"å¤„ç†å›¾ç‰‡: {relative_path}")
        img_start_time = time.time()
        result = None
        update_func = None
        counter_attr = None

        try:
            full_path = self._get_full_image_path(relative_path)

            if not os.path.exists(full_path):
                logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                result = {
                    "image_path": relative_path,
                    "error": f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_path}",
                    "worker_id": worker_id,
                    "has_match": False,
                    "languages": {},
                }
                update_func = self.update_progress_fail
                counter_attr = "error_count"
            else:
                #  è°ƒç”¨OCRè¯†åˆ«
                result = ocr.recognize_image(full_path, predict_save=self.predict_save)
                result['worker_id'] = worker_id
                result['image_path'] = relative_path
                result['time_cost'] = time.time() - img_start_time

                texts = result.get('texts', [])
                error = result.get('error', None)

                if error or not texts:
                    result['has_match'] = False
                    result['languages'] = {}
                    result['confidence'] = 0.0
                    result['processing_time'] = 0
                    update_func = self.update_progress_fail
                    counter_attr = "error_count"
                else:
                    languages = {lang: True for lang in (self.match_languages or ['ch'])
                                 if OCRService.check_language_match(texts, lang)}
                    result['languages'] = languages
                    result['has_match'] = bool(languages)
                    result['confidence'] = result.get('confidence', 0.95)
                    result['processing_time'] = result.get('time_cost', 0)
                    update_func = lambda: self.update_progress_success(result['has_match'])
                    counter_attr = "processed_images"

        except Exception as e:
            logger.error(f"å¤„ç†å›¾ç‰‡å¤±è´¥ {relative_path}: {e}")
            result = {
                "image_path": relative_path,
                "error": str(e),
                "worker_id": worker_id,
                "has_match": False,
                "languages": {},
            }
            update_func = self.update_progress_fail
            counter_attr = "error_count"

        # ç»Ÿä¸€å¤„ç†é˜Ÿåˆ—å’Œè¿›åº¦
        try:
            # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°å·²å¤„ç†å›¾ç‰‡è®¡æ•°
            with self.lock:
                self.processed_images += 1

            self.result_queue.put(result)
            if counter_attr:
                with self.lock:
                    setattr(self, counter_attr, getattr(self, counter_attr) + 1)
            if update_func:
                update_func()
        except Exception as e:
            logger.error(f"result_queue.putå¤±è´¥: {e}")

    def _worker_thread(self, worker_id: int):
        """
        å·¥ä½œçº¿ç¨‹å‡½æ•°

        Args:
            worker_id: å·¥ä½œçº¿ç¨‹ID
        """
        logger.warning(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")

        # å¤ç”¨å¯¹åº”ç´¢å¼•çš„OCRå®ä¾‹
        ocr = self.worker_ocrs[worker_id] if worker_id < len(self.worker_ocrs) else None
        if ocr is None:
            logger.error(f"çº¿ç¨‹ {worker_id}: OCRå®ä¾‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯¥çº¿ç¨‹ä»»åŠ¡")
            return

        processed_count = 0

        while self.is_running:
            try:
                try:
                    relative_path = self.image_queue.get(block=False)  # é˜Ÿåˆ—ä¸­å­˜å‚¨çš„æ˜¯ç›¸å¯¹è·¯å¾„
                except queue.Empty:
                    break

                # åªè°ƒç”¨ä¸€æ¬¡å›¾ç‰‡å¤„ç†é€»è¾‘
                self._process_single_image(ocr, relative_path, worker_id)
                processed_count += 1

            except Exception as e:
                self.update_progress_exception()
                logger.error(f"çº¿ç¨‹ {worker_id}: æœªçŸ¥å¼‚å¸¸: {e}")
                logger.error(traceback.format_exc())
                break

        logger.debug(f"çº¿ç¨‹ {worker_id} å®Œæˆï¼Œå¤„ç† {processed_count} å¼ å›¾ç‰‡")

    def _result_collector(self, all_results: List[Dict]):
        """ç»“æœæ”¶é›†çº¿ç¨‹"""
        logger.warning("ç»“æœæ”¶é›†çº¿ç¨‹å¯åŠ¨")
        buffer = []
        # todo æ”¹æˆç™¾åˆ†æ¯”å­˜å›¾ï¼Œä½†æ˜¯å¯èƒ½ä¼šå¯¼è‡´ä¸€æ¬¡æ’å…¥æ•°æ®è¿‡å¤šï¼ˆå¤šä»»åŠ¡æ‰§è¡Œä¼šæ¶‰åŠåˆ°è¡¨é”ï¼‰
        batch_size = settings.CFG.getint('ocr', 'ocr_batch_size', fallback=10)
        flush_interval = settings.CFG.getint('ocr', 'ocr_flush_interval', fallback=3)
        last_flush = time.time()

        while self.is_running or not self.result_queue.empty():
            try:
                try:
                    result = self.result_queue.get(timeout=3.0)
                except queue.Empty:
                    continue
                buffer.append(result)
                # å¤„ç†ç»“æœ
                all_results.append(result)
                # å¦‚æœè®¾ç½®äº†ç»“æœå›è°ƒï¼Œåˆ™è°ƒç”¨
                if self.result_callback:
                    self.result_callback(result)

                # æ»¡è¶³æ‰¹é‡æˆ–å®šæ—¶æ¡ä»¶å°±å†™åº“
                if len(buffer) >= batch_size or (time.time() - last_flush) > flush_interval:
                    if buffer:
                        self.bulk_insert_to_db(buffer)
                        buffer.clear()
                        last_flush = time.time()
                # æ ‡è®°ç»“æœå¤„ç†å®Œæˆ
                self.result_queue.task_done()
            except Exception as e:
                logger.error(f"ç»“æœæ”¶é›†çº¿ç¨‹å¼‚å¸¸: {str(e)}")

        # å¤„ç†å‰©ä½™æœªå†™å…¥çš„æ•°æ®
        if buffer:
            self.bulk_insert_to_db(buffer)
        logger.warning("ç»“æœæ”¶é›†çº¿ç¨‹ç»“æŸ")

    def bulk_insert_to_db(self, results):
        """
        æ‰¹é‡æ’å…¥OCRè¯†åˆ«ç»“æœåˆ°æ•°æ®åº“
        Args:
            results: List[Dict]ï¼Œæ¯ä¸ªdictä¸ºä¸€å¼ å›¾ç‰‡çš„è¯†åˆ«ç»“æœ
        """
        objs = []
        for result in results:
            obj = OCRResult(
                task=self.task,
                image_path=result.get('image_path', '').replace('\\', '/'),
                texts=result.get('texts', []),
                languages=result.get('languages', {}),
                has_match=result.get('has_match', False),
                confidence=result.get('confidence', 0.0),
                processing_time=result.get('processing_time', 0),
                pic_resolution=result.get('pic_resolution', '')
            )
            objs.append(obj)
        if objs:
            OCRResult.objects.bulk_create(objs)
            logger.warning(f"æ‰¹é‡æ’å…¥ {len(objs)} æ¡OCRç»“æœåˆ°æ•°æ®åº“")
        # todo å¦‚æœè€ƒè™‘ç»§ç»­è¯» task æ•°æ®ï¼Œéœ€è¦åœ¨æ­¤å¤„æ›´æ–° self.task å®ä¾‹ç›¸å…³ç»Ÿè®¡å­—æ®µ

    def init_progress(self, total_images: int):
        """åˆå§‹åŒ–ä»»åŠ¡è¿›åº¦"""
        key = f'ai_ocr_progress:{self.task.id}'
        progress_data = {
            'task_id': str(self.task.id), # ä»»åŠ¡ID
            'total': total_images, # æ€»å›¾ç‰‡æ•°
            'executed': 0, # å·²å¤„ç†å›¾ç‰‡æ•°
            'matched': 0, # ç›®æ ‡è¯­è¨€åŒ¹é…å›¾ç‰‡æ•°
            'success': 0, # è¯†åˆ«åˆ°æ–‡æœ¬å†…å®¹å›¾ç‰‡æ•°
            'fail': 0, # æœªè¯†åˆ«åˆ°æ–‡æœ¬å†…å®¹å›¾ç‰‡æ•°
            'exception': 0, # å¤„ç†å¼‚å¸¸å›¾ç‰‡æ•°
            'match_rate': 0.0, # åŒ¹é…ç‡ï¼ˆåç»­åªåœ¨finish_progressä¸­æ›´æ–°ï¼‰
            'status': 'pending', # ä»»åŠ¡çŠ¶æ€
            'worker_nums': self.max_workers, # ï¼Œå·¥ä½œçº¿ç¨‹æ•°
            'start_time': time.time() # ä»»åŠ¡å¼€å§‹æ—¶é—´
        }
        return self.redis_helper.hmset(key, progress_data)

    def update_progress_success(self, result=False):
        """æ›´æ–°æˆåŠŸè®¡æ•°ï¼ˆåŸå­æ“ä½œï¼‰"""
        key = f'ai_ocr_progress:{self.task.id}'
        pipe = self.redis_helper.pipeline()
        pipe.hincrby(key, 'executed', 1)
        pipe.hincrby(key, 'success', 1)
        # åªè¦æœ‰åŒ¹é…ï¼Œmatched è®¡æ•°+1
        if result:
            pipe.hincrby(key, 'matched', 1)
        return pipe.execute()

    def update_progress_fail(self):
        """æ›´æ–°å¤±è´¥è®¡æ•°ï¼ˆåŸå­æ“ä½œï¼‰"""
        key = f'ai_ocr_progress:{self.task.id}'
        pipe = self.redis_helper.pipeline()
        pipe.hincrby(key, 'executed', 1)
        pipe.hincrby(key, 'fail', 1)
        return pipe.execute()

    def update_progress_exception(self):
        """æ›´æ–°å¼‚å¸¸è®¡æ•°ï¼ˆåŸå­æ“ä½œï¼‰"""
        key = f'ai_ocr_progress:{self.task.id}'
        pipe = self.redis_helper.pipeline()
        pipe.hincrby(key, 'executed', 1)
        pipe.hincrby(key, 'exception', 1)
        return pipe.execute()

    def finish_progress(self, status='completed'):
        """å®Œæˆä»»åŠ¡ & taskç»Ÿè®¡ç»“æœæ›´æ–°ï¼ˆåªæŸ¥Redisï¼Œä¸å›è¡¨ï¼‰"""
        key = f'ai_ocr_progress:{self.task.id}'
        progress_data = self.redis_helper.hgetall(key)

        total_images = int(progress_data.get('total', 0))
        executed_images = int(progress_data.get('executed', 0))
        success_images = int(progress_data.get('success', 0))
        fail_images = int(progress_data.get('fail', 0))
        exception_images = int(progress_data.get('exception', 0))
        matched_images = int(progress_data.get('matched', 0))
        match_rate = (matched_images / total_images * 100) if total_images > 0 else 0

        # æ›´æ–° Redis è¿›åº¦çŠ¶æ€
        self.redis_helper.hmset(key, {
            'status': status,
            'end_time': time.time(),
            'match_rate': round(match_rate, 2)
        })

        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€
        self.task.status = status
        self.task.end_time = timezone.now()
        self.task.total_images = total_images
        self.task.processed_images = executed_images
        self.task.success_images = success_images
        self.task.fail_images = fail_images
        self.task.exception_images = exception_images
        self.task.matched_images = matched_images
        self.task.match_rate = round(match_rate, 2)
        self.task.save()

        logger.warning(
            f"ä»»åŠ¡ {self.task.id} å®Œæˆç»Ÿè®¡: "
            f"æ€»è®¡ {total_images} å¼ , æ‰§è¡Œ {executed_images} å¼ , "
            f"æˆåŠŸ {success_images} å¼ , å¤±è´¥ {fail_images} å¼ , "
            f"å¼‚å¸¸ {exception_images} å¼ , åŒ¹é… {matched_images} å¼ , "
            f"åŒ¹é…ç‡ {match_rate:.2f}%"
        )

        return {
            'total_images': total_images,
            'executed_images': executed_images,
            'success_images': success_images,
            'fail_images': fail_images,
            'exception_images': exception_images,
            'matched_images': matched_images,
            'match_rate': round(match_rate, 2),
            'status': status
        }

    def update_status(self, status: str):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        key = f'ai_ocr_progress:{self.task.id}'
        self.redis_helper.hset(key, 'status', status)
        self.task.status = status
        self.task.save()
        logger.warning(f"ä»»åŠ¡ {self.task.id} çŠ¶æ€æ›´æ–°ä¸º: {status}")
        return status

    def get_progress(self):
        """è·å–å½“å‰è¿›åº¦ä¿¡æ¯ï¼ˆå¸¦è®¡ç®—å­—æ®µï¼‰"""
        key = f'ai_ocr_progress:{self.task.id}'
        progress_data = self.redis_helper.hgetall(key)
        total_images = int(progress_data.get('total', 0))
        executed_images = int(progress_data.get('executed', 0))
        matched_images = int(progress_data.get('matched', 0))
        match_rate = (matched_images / total_images * 100) if total_images > 0 else 0

        # è¿”å›å¸¦è®¡ç®—å­—æ®µçš„è¿›åº¦ä¿¡æ¯
        return {
            'total_images': total_images,
            'executed_images': executed_images,
            'matched_images': matched_images,
            'match_rate': round(match_rate, 2),
            'success_images': int(progress_data.get('success', 0)),
            'fail_images': int(progress_data.get('fail', 0)),
            'exception_images': int(progress_data.get('exception', 0)),
            'status': progress_data.get('status', ''),
            'start_time': float(progress_data.get('start_time', 0)),
            'end_time': float(progress_data.get('end_time', 0)),
        }